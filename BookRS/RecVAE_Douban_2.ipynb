{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import bottleneck as bn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "class CompositePrior(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
    "        super(CompositePrior, self).__init__()\n",
    "        \n",
    "        self.mixture_weights = mixture_weights\n",
    "        \n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.mu_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_uniform_prior.data.fill_(10)\n",
    "        \n",
    "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.encoder_old.requires_grad_(False)\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        post_mu, post_logvar = self.encoder_old(x, 0)\n",
    "        \n",
    "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
    "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
    "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
    "        \n",
    "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
    "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
    "        \n",
    "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
    "                \n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x, dropout_rate):\n",
    "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "        x = x / norm[:, None]\n",
    "    \n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "        \n",
    "        h1 = self.ln1(swish(self.fc1(x)))\n",
    "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
    "    \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, user_ratings, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True):\n",
    "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)    \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_pred = self.decoder(z)\n",
    "        \n",
    "        if calculate_loss:\n",
    "            if gamma:\n",
    "                norm = user_ratings.sum(dim=-1)\n",
    "                kl_weight = gamma * norm\n",
    "            elif beta:\n",
    "                kl_weight = beta\n",
    "\n",
    "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
    "            negative_elbo = -(mll - kld)\n",
    "            \n",
    "            return (mll, kld), negative_elbo\n",
    "            \n",
    "        else:\n",
    "            return x_pred\n",
    "\n",
    "    def update_prior(self):\n",
    "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/dawenl/vae_cf\n",
    "def load_train_data(csv_file, n_items, n_users, global_indexing=False):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    \n",
    "    n_users = n_users if global_indexing else tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data\n",
    "\n",
    "def load_tr_te_data(csv_file_tr, csv_file_te, n_items, n_users, global_indexing=False):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    if global_indexing:\n",
    "        start_idx = 0\n",
    "        end_idx = len(unique_uid) - 1\n",
    "    else:\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te\n",
    "\n",
    "def get_data(dataset, global_indexing=False):\n",
    "    unique_sid = list()\n",
    "    with open(os.path.join(dataset, 'unique_sid.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            unique_sid.append(line.strip())\n",
    "    \n",
    "    unique_uid = list()\n",
    "    with open(os.path.join(dataset, 'unique_uid.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            unique_uid.append(line.strip())\n",
    "            \n",
    "    n_items = len(unique_sid)\n",
    "    n_users = len(unique_uid)\n",
    "    \n",
    "    train_data = load_train_data(os.path.join(dataset, 'train.csv'), n_items, n_users, global_indexing=global_indexing)\n",
    "\n",
    "\n",
    "    vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(dataset, 'validation_tr.csv'),\n",
    "                                               os.path.join(dataset, 'validation_te.csv'),\n",
    "                                               n_items, n_users, \n",
    "                                               global_indexing=global_indexing)\n",
    "\n",
    "    test_data_tr, test_data_te = load_tr_te_data(os.path.join(dataset, 'test_tr.csv'),\n",
    "                                                 os.path.join(dataset, 'test_te.csv'),\n",
    "                                                 n_items, n_users, \n",
    "                                                 global_indexing=global_indexing)\n",
    "    \n",
    "    data = train_data, vad_data_tr, vad_data_te, test_data_tr, test_data_te\n",
    "    data = (x.astype('float32') for x in data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def ndcg(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis], idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum() for n in heldout_batch.getnnz(axis=1)])\n",
    "\n",
    "    # Avoid division by zero: Set NDCG to 0 where IDCG is 0\n",
    "    valid_idx = IDCG != 0\n",
    "    NDCG = np.zeros(batch_users)\n",
    "    NDCG[valid_idx] = DCG[valid_idx] / IDCG[valid_idx]\n",
    "\n",
    "    return NDCG\n",
    "\n",
    "\n",
    "def recall(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    intersect = np.logical_and(X_true_binary, X_pred_binary).sum(axis=1).astype(np.float32)\n",
    "    \n",
    "    relevant = X_true_binary.sum(axis=1)\n",
    "    \n",
    "    # Avoid division by zero: if the denominator is zero, recall can be set to zero (or some other value)\n",
    "    recall = np.where(relevant > 0, intersect / relevant, 0.0)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    Precision@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    - X_pred: Predictive score matrix, where each row represents a user and each column represents an item\n",
    "    - heldout_batch: Real user-item interaction data, again, each row represents a user and each column represents an item\n",
    "    - k: Number of top recommendations considered\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    \n",
    "    # Find the top k highest predicted scores for each user using argpartition\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    # Create a matrix that is all False and then set the top k highest scores to True\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    # Converts heldout_batch to a binary format indicating whether the user is interested in the item or not\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    # Calculate the number of correct predictions for each user\n",
    "    true_positives = np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)\n",
    "    # Calculate and return precision@k\n",
    "    precision_at_k = true_positives.astype(np.float32) / k\n",
    "    return precision_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = r'C:\\Users\\FOMO\\Desktop\\Proj\\Dataset\\Douban\\processed_data_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 600\n",
    "latent_dim = 200\n",
    "batch_size = 500\n",
    "beta = 0.5\n",
    "gamma = 0.035\n",
    "n_epochs = 50  # 训练周期数\n",
    "not_alternating = False  # 是否使用交替训练\n",
    "n_enc_epochs = 3  # 编码器训练周期数\n",
    "n_dec_epochs = 1  # 解码器训练周期数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(processed_dataset)\n",
    "train_data, valid_in_data, valid_out_data, test_in_data, test_out_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x187d3279ad0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "    \n",
    "    total_samples = data_in.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "    \n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "    \n",
    "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_in, data_out)\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, device, idx, data_in, data_out=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_in = data_in\n",
    "        self._data_out = data_out\n",
    "    \n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "    \n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "        \n",
    "    def get_ratings(self, is_out=False):\n",
    "        data = self._data_out if is_out else self._data_in\n",
    "        return data[self._idx]\n",
    "    \n",
    "    def get_ratings_to_dev(self, is_out=False):\n",
    "        return torch.Tensor(\n",
    "            self.get_ratings(is_out).toarray()\n",
    "        ).to(self._device)\n",
    "\n",
    "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
    "    metrics = deepcopy(metrics)\n",
    "    model.eval()\n",
    "    \n",
    "    for m in metrics:\n",
    "        m['score'] = []\n",
    "    \n",
    "    for batch in generate(batch_size=batch_size,\n",
    "                          device=device,\n",
    "                          data_in=data_in,\n",
    "                          data_out=data_out,\n",
    "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
    "                         ):\n",
    "        \n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_out = batch.get_ratings(is_out=True)\n",
    "    \n",
    "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
    "        \n",
    "        if not (data_in is data_out):\n",
    "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "            \n",
    "        for m in metrics:\n",
    "            m['score'].append(m['metric'](ratings_pred, ratings_out, k=m['k']))\n",
    "\n",
    "    for m in metrics:\n",
    "        m['score'] = np.concatenate(m['score']).mean()\n",
    "        \n",
    "    return [x['score'] for x in metrics]\n",
    "\n",
    "def run(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=True):\n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            _, loss = model(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
    "            loss.backward()\n",
    "            \n",
    "            for optimizer in opts:\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'latent_dim': latent_dim,\n",
    "    'input_dim': train_data.shape[1]\n",
    "}\n",
    "metrics = [{'metric': ndcg, 'k': 100}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ndcg = -np.inf\n",
    "train_scores, valid_scores = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(**model_kwargs).to(device)\n",
    "model_best = VAE(**model_kwargs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_kwargs = {\n",
    "    'model': model,\n",
    "    'train_data': train_data,\n",
    "    'batch_size': batch_size,\n",
    "    'beta': beta,\n",
    "    'gamma': gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_params = set(model.decoder.parameters())\n",
    "encoder_params = set(model.encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_encoder = optim.Adam(encoder_params, lr=0.001)\n",
    "optimizer_decoder = optim.Adam(decoder_params, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | valid ndcg@100: 0.0129 | best valid: 0.0129 | train ndcg@100: 0.0248\n",
      "epoch 1 | valid ndcg@100: 0.0164 | best valid: 0.0164 | train ndcg@100: 0.0298\n",
      "epoch 2 | valid ndcg@100: 0.0263 | best valid: 0.0263 | train ndcg@100: 0.0503\n",
      "epoch 3 | valid ndcg@100: 0.0561 | best valid: 0.0561 | train ndcg@100: 0.1235\n",
      "epoch 4 | valid ndcg@100: 0.0910 | best valid: 0.0910 | train ndcg@100: 0.2476\n",
      "epoch 5 | valid ndcg@100: 0.1174 | best valid: 0.1174 | train ndcg@100: 0.3767\n",
      "epoch 6 | valid ndcg@100: 0.1346 | best valid: 0.1346 | train ndcg@100: 0.4856\n",
      "epoch 7 | valid ndcg@100: 0.1450 | best valid: 0.1450 | train ndcg@100: 0.5845\n",
      "epoch 8 | valid ndcg@100: 0.1513 | best valid: 0.1513 | train ndcg@100: 0.6633\n",
      "epoch 9 | valid ndcg@100: 0.1547 | best valid: 0.1547 | train ndcg@100: 0.7301\n",
      "epoch 10 | valid ndcg@100: 0.1572 | best valid: 0.1572 | train ndcg@100: 0.7809\n",
      "epoch 11 | valid ndcg@100: 0.1562 | best valid: 0.1572 | train ndcg@100: 0.8235\n",
      "epoch 12 | valid ndcg@100: 0.1576 | best valid: 0.1576 | train ndcg@100: 0.8540\n",
      "epoch 13 | valid ndcg@100: 0.1579 | best valid: 0.1579 | train ndcg@100: 0.8756\n",
      "epoch 14 | valid ndcg@100: 0.1585 | best valid: 0.1585 | train ndcg@100: 0.8931\n",
      "epoch 15 | valid ndcg@100: 0.1586 | best valid: 0.1586 | train ndcg@100: 0.9040\n",
      "epoch 16 | valid ndcg@100: 0.1595 | best valid: 0.1595 | train ndcg@100: 0.9125\n",
      "epoch 17 | valid ndcg@100: 0.1595 | best valid: 0.1595 | train ndcg@100: 0.9198\n",
      "epoch 18 | valid ndcg@100: 0.1594 | best valid: 0.1595 | train ndcg@100: 0.9255\n",
      "epoch 19 | valid ndcg@100: 0.1595 | best valid: 0.1595 | train ndcg@100: 0.9307\n",
      "epoch 20 | valid ndcg@100: 0.1594 | best valid: 0.1595 | train ndcg@100: 0.9340\n",
      "epoch 21 | valid ndcg@100: 0.1604 | best valid: 0.1604 | train ndcg@100: 0.9387\n",
      "epoch 22 | valid ndcg@100: 0.1605 | best valid: 0.1605 | train ndcg@100: 0.9428\n",
      "epoch 23 | valid ndcg@100: 0.1612 | best valid: 0.1612 | train ndcg@100: 0.9451\n",
      "epoch 24 | valid ndcg@100: 0.1614 | best valid: 0.1614 | train ndcg@100: 0.9481\n",
      "epoch 25 | valid ndcg@100: 0.1618 | best valid: 0.1618 | train ndcg@100: 0.9503\n",
      "epoch 26 | valid ndcg@100: 0.1615 | best valid: 0.1618 | train ndcg@100: 0.9525\n",
      "epoch 27 | valid ndcg@100: 0.1612 | best valid: 0.1618 | train ndcg@100: 0.9551\n",
      "epoch 28 | valid ndcg@100: 0.1614 | best valid: 0.1618 | train ndcg@100: 0.9557\n",
      "epoch 29 | valid ndcg@100: 0.1610 | best valid: 0.1618 | train ndcg@100: 0.9586\n",
      "epoch 30 | valid ndcg@100: 0.1614 | best valid: 0.1618 | train ndcg@100: 0.9588\n",
      "epoch 31 | valid ndcg@100: 0.1624 | best valid: 0.1624 | train ndcg@100: 0.9603\n",
      "epoch 32 | valid ndcg@100: 0.1613 | best valid: 0.1624 | train ndcg@100: 0.9610\n",
      "epoch 33 | valid ndcg@100: 0.1616 | best valid: 0.1624 | train ndcg@100: 0.9622\n",
      "epoch 34 | valid ndcg@100: 0.1618 | best valid: 0.1624 | train ndcg@100: 0.9624\n",
      "epoch 35 | valid ndcg@100: 0.1627 | best valid: 0.1627 | train ndcg@100: 0.9648\n",
      "epoch 36 | valid ndcg@100: 0.1620 | best valid: 0.1627 | train ndcg@100: 0.9651\n",
      "epoch 37 | valid ndcg@100: 0.1624 | best valid: 0.1627 | train ndcg@100: 0.9659\n",
      "epoch 38 | valid ndcg@100: 0.1622 | best valid: 0.1627 | train ndcg@100: 0.9665\n",
      "epoch 39 | valid ndcg@100: 0.1633 | best valid: 0.1633 | train ndcg@100: 0.9677\n",
      "epoch 40 | valid ndcg@100: 0.1622 | best valid: 0.1633 | train ndcg@100: 0.9680\n",
      "epoch 41 | valid ndcg@100: 0.1622 | best valid: 0.1633 | train ndcg@100: 0.9695\n",
      "epoch 42 | valid ndcg@100: 0.1616 | best valid: 0.1633 | train ndcg@100: 0.9700\n",
      "epoch 43 | valid ndcg@100: 0.1614 | best valid: 0.1633 | train ndcg@100: 0.9706\n",
      "epoch 44 | valid ndcg@100: 0.1614 | best valid: 0.1633 | train ndcg@100: 0.9708\n",
      "epoch 45 | valid ndcg@100: 0.1612 | best valid: 0.1633 | train ndcg@100: 0.9710\n",
      "epoch 46 | valid ndcg@100: 0.1611 | best valid: 0.1633 | train ndcg@100: 0.9722\n",
      "epoch 47 | valid ndcg@100: 0.1610 | best valid: 0.1633 | train ndcg@100: 0.9737\n",
      "epoch 48 | valid ndcg@100: 0.1609 | best valid: 0.1633 | train ndcg@100: 0.9742\n",
      "epoch 49 | valid ndcg@100: 0.1610 | best valid: 0.1633 | train ndcg@100: 0.9742\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    if not_alternating:\n",
    "        run(opts=[optimizer_encoder, optimizer_decoder], n_epochs=1, dropout_rate=0.5, **learning_kwargs)\n",
    "    else:\n",
    "        run(opts=[optimizer_encoder], n_epochs=n_enc_epochs, dropout_rate=0.5, **learning_kwargs)\n",
    "        model.update_prior()\n",
    "        run(opts=[optimizer_decoder], n_epochs=n_dec_epochs, dropout_rate=0, **learning_kwargs)\n",
    "\n",
    "    train_scores.append(\n",
    "        evaluate(model, train_data, train_data, metrics, 0.01)[0]\n",
    "    )\n",
    "    valid_scores.append(\n",
    "        evaluate(model, valid_in_data, valid_out_data, metrics, 1)[0]\n",
    "    )\n",
    "    \n",
    "    if valid_scores[-1] > best_ndcg:\n",
    "        best_ndcg = valid_scores[-1]\n",
    "        model_best.load_state_dict(deepcopy(model.state_dict()))\n",
    "    \n",
    "    print(f'epoch {epoch} | valid ndcg@100: {valid_scores[-1]:.4f} | ' +\n",
    "          f'best valid: {best_ndcg:.4f} | train ndcg@100: {train_scores[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = [\n",
    "    {'metric': recall, 'k': 5},\n",
    "    {'metric': recall, 'k': 10},\n",
    "    {'metric': recall, 'k': 20},\n",
    "    {'metric': recall, 'k': 50},\n",
    "    {'metric': recall, 'k': 100},\n",
    "    {'metric': precision, 'k': 5},\n",
    "    {'metric': precision, 'k': 10},\n",
    "    {'metric': precision, 'k': 20},\n",
    "    {'metric': precision, 'k': 50},\n",
    "    {'metric': precision, 'k': 100},\n",
    "    {'metric': ndcg, 'k': 5},\n",
    "    {'metric': ndcg, 'k': 10},\n",
    "    {'metric': ndcg, 'k': 20},\n",
    "    {'metric': ndcg, 'k': 50},\n",
    "    {'metric': ndcg, 'k': 100}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FOMO\\AppData\\Local\\Temp\\ipykernel_30772\\3763803702.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  recall = np.where(relevant > 0, intersect / relevant, 0.0)\n"
     ]
    }
   ],
   "source": [
    "final_scores = evaluate(model_best, test_in_data, test_out_data, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5:\t0.1214\n",
      "recall@10:\t0.1543\n",
      "recall@20:\t0.1903\n",
      "recall@50:\t0.2463\n",
      "recall@100:\t0.2955\n",
      "precision@5:\t0.0457\n",
      "precision@10:\t0.0308\n",
      "precision@20:\t0.0201\n",
      "precision@50:\t0.0112\n",
      "precision@100:\t0.0070\n",
      "ndcg@5:\t0.1084\n",
      "ndcg@10:\t0.1187\n",
      "ndcg@20:\t0.1294\n",
      "ndcg@50:\t0.1435\n",
      "ndcg@100:\t0.1537\n"
     ]
    }
   ],
   "source": [
    "for metric, score in zip(test_metrics, final_scores):\n",
    "    print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
