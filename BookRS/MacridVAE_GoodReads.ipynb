{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(inputs, logits):\n",
    "    return torch.mean(torch.sum(-logits * inputs, dim=1))\n",
    "\n",
    "def kl_loss(mu, logvar):\n",
    "    return torch.mean(0.5 * torch.sum(\n",
    "        torch.exp(logvar) + mu ** 2 - 1 - logvar, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisenVAE(nn.Module):\n",
    "    def __init__(self, M, K, D, tau, dropout):\n",
    "        super(DisenVAE, self).__init__()\n",
    "\n",
    "        self.M = M\n",
    "        self.H = D * 3\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        self.tau = tau\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.M, self.H),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.H, self.D * 2)\n",
    "        )\n",
    "        self.items = Parameter(torch.Tensor(self.M, self.D))\n",
    "        self.cores = Parameter(torch.Tensor(self.K, self.D))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        init.xavier_normal_(self.items)\n",
    "        init.xavier_normal_(self.cores)\n",
    "\n",
    "\n",
    "    def cluster(self):\n",
    "        items = F.normalize(self.items, dim=1)      # M * D\n",
    "        cores = F.normalize(self.cores, dim=1)      # K * D\n",
    "        cates = torch.mm(items, cores.t()) / self.tau\n",
    "        cates = F.softmax(cates, dim=1)             # M * K\n",
    "        return items, cores, cates\n",
    "\n",
    "\n",
    "    def encode(self, X, cates):\n",
    "        n = X.shape[0]\n",
    "        X = self.drop(X)\n",
    "        X = X.view(n, 1, self.M) *  \\\n",
    "            cates.t().expand(n, self.K, self.M)     # n * K * M\n",
    "        X = X.reshape(n * self.K, self.M)           # (n * K) * M\n",
    "        h = self.encoder(X)                         # (n * K) * D * 2\n",
    "        mu, logvar = h[:, :self.D], h[:, self.D:]   # (n * k) * D\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "    def decode(self, z, items, cates):\n",
    "        n = z.shape[0] // self.K\n",
    "        z = F.normalize(z, dim=1)                   # (n * K) * D\n",
    "        logits = torch.mm(z, items.t()) / self.tau  # (n * K) * M\n",
    "        probs = torch.exp(logits)                   # (n * K) * M\n",
    "        probs = torch.sum(probs.view(n, self.K, self.M) * \\\n",
    "                cates.t().expand(n, self.K, self.M), dim=1)\n",
    "        logits = torch.log(probs)\n",
    "        logits = F.log_softmax(logits, dim=1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + std * eps\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        items, cores, cates = self.cluster()\n",
    "        mu, logvar = self.encode(X, cates)\n",
    "        z = self.sample(mu, logvar)\n",
    "        logits = self.decode(z, items, cates)\n",
    "        return logits, mu, logvar, None, None, None\n",
    "    \n",
    "\n",
    "    def loss_fn(self, X, X_logits, X_mu, X_logvar,\n",
    "                A, A_logits, A_mu, A_logvar, anneal):\n",
    "        return recon_loss(X, X_logits) + anneal * kl_loss(X_mu, X_logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function: Normalized discounted cumulative gain (NDCG@k)  Recall@k and Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_kth(outputs, labels, k=100):\n",
    "    _, preds = torch.topk(outputs, k)               # sorted top k index of outputs\n",
    "    _, facts = torch.topk(labels, k)                # min(k, labels.nnz(dim=1))\n",
    "    rows = torch.arange(labels.shape[0]).view(-1, 1)\n",
    "\n",
    "    tp = 1.0 / torch.log2(torch.arange(2, k + 2).float())\n",
    "    dcg = torch.sum(tp * labels[rows, preds], dim=1)\n",
    "    idcg = torch.sum(tp * labels[rows, facts], dim=1)\n",
    "    ndcg = dcg / idcg\n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def recall_kth(outputs, labels, k=50):\n",
    "    _, preds = torch.topk(outputs, k, sorted=False) # top k index\n",
    "    rows = torch.arange(labels.shape[0]).view(-1, 1)\n",
    "\n",
    "    recall = torch.sum(labels[rows, preds], dim=1) \\\n",
    "           / torch.min(torch.Tensor([k]), torch.sum(labels, dim=1))\n",
    "    recall[torch.isnan(recall)] = 0\n",
    "    return recall\n",
    "\n",
    "def precision_kth(outputs, labels, k=50):\n",
    "    \"\"\"\n",
    "    Compute Precision@k: the proportion of positive class predictions in the top-k predictions\n",
    "    that are correct.\n",
    "    \n",
    "    Args:\n",
    "    - outputs (torch.Tensor): The model outputs (scores or probabilities) for each class.\n",
    "    - labels (torch.Tensor): The ground truth labels (binary) for each sample.\n",
    "    - k (int): The number of top predictions to consider for computing precision.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: Precision@k for each sample in the batch.\n",
    "    \"\"\"\n",
    "    # Get the index of the top k highest scoring predictions\n",
    "    _, preds = torch.topk(outputs, k, sorted=False)  # top k index\n",
    "    \n",
    "    # Generate a row index for each sample\n",
    "    rows = torch.arange(labels.shape[0]).view(-1, 1)\n",
    "\n",
    "    # Compute Precision@k\n",
    "    # Get the actual labels by indexing and calculate the number of correct predictions for each sample\n",
    "    correct_preds = torch.sum(labels[rows, preds], dim=1)\n",
    "    precision = correct_preds / k\n",
    "    precision[torch.isnan(precision)] = 0\n",
    "    \n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/validation data, hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-processed training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = r'processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join(processed_dataset, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = list()\n",
    "with open(os.path.join(processed_dataset, 'unique_uid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_uid.append(line.strip())\n",
    "n_users = len(unique_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(csv_file, n_items):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                              (rows, cols)), dtype='float64',\n",
    "                             shape=(users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_train_data(os.path.join(processed_dataset, 'train.csv'), n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te, n_items):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                                 (rows_tr, cols_tr)), dtype='float64',\n",
    "                                shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                 (rows_te, cols_te)), dtype='float64',\n",
    "                                shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_tr, vad_data_te = load_tr_te_data(\n",
    "        os.path.join(processed_dataset, 'validation_tr.csv'),\n",
    "        os.path.join(processed_dataset, 'validation_te.csv'),\n",
    "        n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_data_tr, tst_data_te = load_tr_te_data(\n",
    "        os.path.join(processed_dataset, 'test_tr.csv'),\n",
    "        os.path.join(processed_dataset, 'test_te.csv'),\n",
    "        n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_items == train_data.shape[1]\n",
    "assert n_items == vad_data_tr.shape[1]\n",
    "assert n_items == vad_data_te.shape[1]\n",
    "assert n_items == tst_data_tr.shape[1]\n",
    "assert n_items == tst_data_te.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "weight_decay = 1e-4 # weight decay coefficient\n",
    "dropout = 0.5\n",
    "beta = 0.8\n",
    "kfac = 7\n",
    "dfac = 200\n",
    "tau = 0.1\n",
    "batch_size = 800\n",
    "epochs = 50\n",
    "total_anneal_steps = 200000 # the total number of gradient updates for annealing\n",
    "anneal_cap = 0.2 \n",
    "seed = 98765 # random seed\n",
    "log_interval = 100\n",
    "save = r'model.pt' # path to save the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f627362a8b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed) # Set the random seed manually for reproductibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  \n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  \n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(model, N, M, K, D, tau, dropout):\n",
    "    if model == 'DisenVAE':\n",
    "        return DisenVAE(M, K, D, tau, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DisenVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=72226, out_features=600, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=600, out_features=400, bias=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = load_net('DisenVAE', n_users, n_items, kfac, dfac, \n",
    "               tau, dropout)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = net.loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a MacridVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg100_values = []\n",
    "\n",
    "empty_data = sparse.csr_matrix(train_data.shape, dtype=float)\n",
    "tr_data = sparse.vstack([train_data, vad_data_tr, tst_data_tr])\n",
    "te_data = sparse.vstack([empty_data, vad_data_te, tst_data_te])\n",
    "\n",
    "n_train = train_data.shape[0]\n",
    "n_valid = vad_data_tr.shape[0]\n",
    "n_test  = tst_data_tr.shape[0]\n",
    "train_idx = range(n_train)\n",
    "valid_idx = range(n_train, n_train + n_valid)\n",
    "test_idx  = range(n_train + n_valid, n_train + n_valid + n_test)\n",
    "\n",
    "n_batches = int(np.ceil(n_train / batch_size))\n",
    "update = 0\n",
    "anneals = 500 * n_batches\n",
    "best_n100 = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, idx):\n",
    "    net.eval()\n",
    "    n_test = len(idx)\n",
    "    metrics = {'ndcg': {}, 'recall': {}, 'precision': {}}\n",
    "    ks = [5, 10, 20, 50, 100]\n",
    "\n",
    "    # Initialising the metrics store\n",
    "    for k in ks:\n",
    "        metrics['ndcg'][k] = []\n",
    "        metrics['recall'][k] = []\n",
    "        metrics['precision'][k] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, n_test, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_test)\n",
    "            X_tr = tr_data[idx[start_idx: end_idx]]\n",
    "            X_te = te_data[idx[start_idx: end_idx]]\n",
    "            X_tr = torch.Tensor(X_tr.toarray()).to(device)\n",
    "            X_te = torch.Tensor(X_te.toarray())\n",
    "            A = None\n",
    "            X_tr_logits, _, _, _, _, _ = net(X_tr, A)\n",
    "\n",
    "            X_tr_logits[torch.nonzero(X_tr, as_tuple=True)] = float('-inf')\n",
    "            X_tr_logits = X_tr_logits.cpu()\n",
    "\n",
    "            # Calculation of indicators\n",
    "            for k in ks:\n",
    "                metrics['ndcg'][k].append(ndcg_kth(X_tr_logits, X_te, k=k))\n",
    "                metrics['recall'][k].append(recall_kth(X_tr_logits, X_te, k=k))\n",
    "                metrics['precision'][k].append(precision_kth(X_tr_logits, X_te, k=k))\n",
    "\n",
    "    # Converting a list of indicators to an average\n",
    "    for metric in metrics:\n",
    "        for k in ks:\n",
    "            metrics[metric][k] = torch.mean(torch.cat(metrics[metric][k])).item()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0] loss: 2246.780\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 71.47s | ndcg@100 0.241 | recall@20 0.201 | recall@50 0.191\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  1] loss: 1998.250\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 71.21s | ndcg@100 0.306 | recall@20 0.263 | recall@50 0.247\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  2] loss: 1927.826\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 71.13s | ndcg@100 0.349 | recall@20 0.306 | recall@50 0.283\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  3] loss: 1892.532\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 70.95s | ndcg@100 0.377 | recall@20 0.337 | recall@50 0.307\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  4] loss: 1873.408\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 71.33s | ndcg@100 0.388 | recall@20 0.348 | recall@50 0.316\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  5] loss: 1862.966\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 70.96s | ndcg@100 0.395 | recall@20 0.356 | recall@50 0.322\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  6] loss: 1856.392\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 71.05s | ndcg@100 0.400 | recall@20 0.360 | recall@50 0.326\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  7] loss: 1852.350\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 71.27s | ndcg@100 0.403 | recall@20 0.364 | recall@50 0.330\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  8] loss: 1849.883\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 70.71s | ndcg@100 0.405 | recall@20 0.367 | recall@50 0.332\n",
      "-----------------------------------------------------------------------------------------\n",
      "[  9] loss: 1847.528\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 71.23s | ndcg@100 0.407 | recall@20 0.368 | recall@50 0.333\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 10] loss: 1846.216\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 71.21s | ndcg@100 0.409 | recall@20 0.370 | recall@50 0.335\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 11] loss: 1845.588\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 71.59s | ndcg@100 0.408 | recall@20 0.370 | recall@50 0.334\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 14] loss: 1843.838\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 71.21s | ndcg@100 0.412 | recall@20 0.374 | recall@50 0.338\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 15] loss: 1843.824\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 71.11s | ndcg@100 0.412 | recall@20 0.374 | recall@50 0.338\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 16] loss: 1843.563\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 71.11s | ndcg@100 0.412 | recall@20 0.374 | recall@50 0.338\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 17] loss: 1843.415\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 71.32s | ndcg@100 0.414 | recall@20 0.376 | recall@50 0.340\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 18] loss: 1843.503\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 72.06s | ndcg@100 0.413 | recall@20 0.375 | recall@50 0.339\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 19] loss: 1843.321\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 71.74s | ndcg@100 0.413 | recall@20 0.375 | recall@50 0.339\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 20] loss: 1843.546\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 71.70s | ndcg@100 0.414 | recall@20 0.376 | recall@50 0.340\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 21] loss: 1843.860\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 72.04s | ndcg@100 0.414 | recall@20 0.375 | recall@50 0.339\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 22] loss: 1843.934\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 71.63s | ndcg@100 0.414 | recall@20 0.376 | recall@50 0.341\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 23] loss: 1844.147\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 71.09s | ndcg@100 0.414 | recall@20 0.376 | recall@50 0.341\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 24] loss: 1844.272\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 71.31s | ndcg@100 0.414 | recall@20 0.376 | recall@50 0.340\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 25] loss: 1844.496\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 71.20s | ndcg@100 0.416 | recall@20 0.377 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 26] loss: 1844.821\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 70.64s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.341\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 27] loss: 1844.939\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 70.70s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.341\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 28] loss: 1845.308\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 71.46s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.341\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 29] loss: 1845.319\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 70.64s | ndcg@100 0.415 | recall@20 0.378 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 30] loss: 1845.904\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 70.94s | ndcg@100 0.416 | recall@20 0.378 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 31] loss: 1846.307\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 70.56s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 32] loss: 1846.707\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 70.67s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 33] loss: 1846.906\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 71.25s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 34] loss: 1847.204\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 71.04s | ndcg@100 0.415 | recall@20 0.377 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 35] loss: 1847.265\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 70.72s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 36] loss: 1848.209\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 69.87s | ndcg@100 0.414 | recall@20 0.375 | recall@50 0.341\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 37] loss: 1848.739\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 70.45s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 38] loss: 1849.032\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 70.25s | ndcg@100 0.415 | recall@20 0.375 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 39] loss: 1849.159\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 70.74s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 40] loss: 1849.586\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 70.09s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 41] loss: 1849.542\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 70.61s | ndcg@100 0.415 | recall@20 0.375 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 42] loss: 1850.089\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 70.08s | ndcg@100 0.415 | recall@20 0.376 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 43] loss: 1850.343\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 69.64s | ndcg@100 0.414 | recall@20 0.375 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 44] loss: 1850.808\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 69.70s | ndcg@100 0.415 | recall@20 0.375 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 45] loss: 1851.389\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 70.48s | ndcg@100 0.414 | recall@20 0.374 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 46] loss: 1852.131\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 69.99s | ndcg@100 0.415 | recall@20 0.375 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 47] loss: 1852.137\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 69.61s | ndcg@100 0.414 | recall@20 0.375 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 48] loss: 1852.220\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 70.23s | ndcg@100 0.414 | recall@20 0.374 | recall@50 0.341\n",
      "-----------------------------------------------------------------------------------------\n",
      "[ 49] loss: 1852.969\t-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 69.92s | ndcg@100 0.414 | recall@20 0.374 | recall@50 0.342\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        t = time.time()\n",
    "        \n",
    "        train_idx = np.random.permutation(train_idx)\n",
    "        \n",
    "        for start_idx in range(0, n_train, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_train)\n",
    "            X = train_data[train_idx[start_idx: end_idx]]\n",
    "            X = torch.Tensor(X.toarray()).to(device)  # users-items matrix\n",
    "            A = None\n",
    "            optimizer.zero_grad()\n",
    "            X_logits, X_mu, X_logvar, A_logits, A_mu, A_logvar = net(X, A)\n",
    "            anneal = min(beta, update / anneals)\n",
    "            loss = criterion(X, X_logits, X_mu, X_logvar, A, A_logits, A_mu, A_logvar, anneal)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            update += 1\n",
    "        \n",
    "        print('[%3d] loss: %.3f' % (epoch, running_loss / n_batches), end='\\t', flush=True)\n",
    "        # Evaluation \n",
    "        metrics = evaluate(net, valid_idx)\n",
    "        ndcg100_values.append(metrics['ndcg'][100]) \n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:4.2f}s | '\n",
    "            'ndcg@100 {:5.3f} | recall@20 {:5.3f} | recall@50 {:5.3f}'.format(\n",
    "                epoch, time.time() - epoch_start_time, \n",
    "                metrics['ndcg'][100], metrics['recall'][20], metrics['recall'][50]))\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save the model if the n100 is the best we've seen so far.\n",
    "        if metrics['ndcg'][100] > best_n100:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(net, f)\n",
    "            best_n100 = metrics['ndcg'][100]    \n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ndcg100_values, label='NDCG@100')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NDCG@100')\n",
    "plt.title('NDCG@100 on Validation Set Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training |\n",
      "| ndcg@5 0.535 | recall@5 0.517 | precision@5 0.517\n",
      "| ndcg@10 0.485 | recall@10 0.454 | precision@10 0.454\n",
      "| ndcg@20 0.418 | recall@20 0.374 | precision@20 0.374\n",
      "| ndcg@50 0.377 | recall@50 0.343 | precision@50 0.257\n",
      "| ndcg@100 0.416 | recall@100 0.421 | precision@100 0.179\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "metrics = evaluate(net, test_idx)\n",
    "\n",
    "print('=' * 89)\n",
    "print('| End of training |')\n",
    "for k in [5, 10, 20, 50, 100]:\n",
    "    print('| ndcg@{} {:5.3f} | recall@{} {:5.3f} | precision@{} {:5.3f}'.format(\n",
    "        k, metrics['ndcg'][k], k, metrics['recall'][k], k, metrics['precision'][k]))\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
